Input args: Namespace(dname='CIFAR10', seeds=[0], architecture='CIFAR10_CNN', accountant='rdp', individualize='clipping', log_iteration=100, lr=0.1, momentum=0.5, epochs=60, n_workers=6, batch_size=1024, max_physical_batch_size=1024, delta=1e-05, budgets=[1.0, 2.0, 3.0], ratios=[0.54, 0.37, 0.09], max_grad_norm=1.8, noise_multiplier=3.17792, weights=None, adapt_weights_to_budgets=True, use_cuda='True', save_path='../cifar1_results/clipping/CIFAR10/epochs_60_batch_1024_lr_0.1_max_grad_norm_1.8_budgets_1.0_2.0_3.0_ratios_0.54_0.37_0.09_seeds_0', mode='mia', accuracy_log='accuracy.log', assign_budget='random', class_budgets=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], mia_ndata=50000, mia_count=0, allow_excess=False, save_model='True')
seed: 0,   max_iteration: 2930,   1 epoch ~= 49 iterations
seed: 0,   max_iteration: 2930,   1 epoch ~= 49 iterations
/local/scratch/manwa22/MIAs_on_Personalized_DP_models/experiments/../opacus/opacus/privacy_engine.py:126: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/local/scratch/manwa22/MIAs_on_Personalized_DP_models/experiments/../opacus/opacus/accountants/analysis/rdp.py:74: RuntimeWarning: invalid value encountered in scalar subtract
  return math.log1p(math.exp(a - b)) + b  # log1p(x) = log(x + 1)
Initializing privacy parameters:   max_grad_norm=1.8,   sample_rate=0.02040816326530612,   noise_multiplier 3.5613159179687504,   individual max_grad_norms=[1.4033602351683592, 2.549210679611651, 3.51262474916388]
/local/scratch/manwa22/anaconda3/envs/idp/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
iteration: 0,   train accuracy: 10.38,   test accuracy: 10.27,   loss: 2.30579,   DP costs: [0.036, 0.141, 0.295],   average batch-size: 942,   best alpha: [161.29, 49.13, 26.2],   time: 1
iteration: 100,   train accuracy: 28.24,   test accuracy: 28.75,   loss: 1.90823,   DP costs: [0.167, 0.344, 0.538],   average batch-size: 1091,   best alpha: [77.58, 40.32, 24.38],   time: 12
iteration: 200,   train accuracy: 34.1,   test accuracy: 34.58,   loss: 1.88883,   DP costs: [0.24, 0.486, 0.741],   average batch-size: 1015,   best alpha: [57.9, 30.83, 20.91],   time: 21
iteration: 300,   train accuracy: 38.95,   test accuracy: 38.98,   loss: 1.78073,   DP costs: [0.296, 0.598, 0.906],   average batch-size: 1010,   best alpha: [48.46, 26.08, 17.95],   time: 31
iteration: 400,   train accuracy: 41.52,   test accuracy: 42.83,   loss: 1.63482,   DP costs: [0.345, 0.693, 1.047],   average batch-size: 1042,   best alpha: [42.77, 23.16, 16.1],   time: 40
iteration: 500,   train accuracy: 43.92,   test accuracy: 44.19,   loss: 1.55781,   DP costs: [0.388, 0.778, 1.173],   average batch-size: 1040,   best alpha: [38.68, 21.16, 14.79],   time: 49
iteration: 600,   train accuracy: 45.28,   test accuracy: 45.03,   loss: 1.55834,   DP costs: [0.427, 0.856, 1.288],   average batch-size: 1027,   best alpha: [35.69, 19.56, 13.75],   time: 58
iteration: 700,   train accuracy: 46.03,   test accuracy: 46.44,   loss: 1.59502,   DP costs: [0.464, 0.928, 1.395],   average batch-size: 1027,   best alpha: [33.34, 18.38, 12.94],   time: 67
iteration: 800,   train accuracy: 47.7,   test accuracy: 47.97,   loss: 1.5477,   DP costs: [0.498, 0.996, 1.495],   average batch-size: 991,   best alpha: [31.4, 17.33, 12.27],   time: 75
iteration: 900,   train accuracy: 47.54,   test accuracy: 48.02,   loss: 1.43268,   DP costs: [0.53, 1.06, 1.591],   average batch-size: 1079,   best alpha: [29.82, 16.55, 11.72],   time: 84
iteration: 1000,   train accuracy: 48.04,   test accuracy: 49.24,   loss: 1.44001,   DP costs: [0.561, 1.12, 1.681],   average batch-size: 1002,   best alpha: [28.54, 15.8, 11.25],   time: 93
iteration: 1100,   train accuracy: 49.85,   test accuracy: 49.49,   loss: 1.32599,   DP costs: [0.59, 1.179, 1.768],   average batch-size: 971,   best alpha: [27.32, 15.2, 10.83],   time: 101
iteration: 1200,   train accuracy: 51.14,   test accuracy: 50.81,   loss: 1.56308,   DP costs: [0.618, 1.234, 1.851],   average batch-size: 971,   best alpha: [26.25, 14.62, 10.48],   time: 110
iteration: 1300,   train accuracy: 50.0,   test accuracy: 49.58,   loss: 1.72495,   DP costs: [0.645, 1.288, 1.931],   average batch-size: 1020,   best alpha: [25.33, 14.18, 10.13],   time: 118
iteration: 1400,   train accuracy: 52.48,   test accuracy: 52.46,   loss: 1.38541,   DP costs: [0.671, 1.34, 2.009],   average batch-size: 989,   best alpha: [24.54, 13.7, 9.83],   time: 127
iteration: 1500,   train accuracy: 53.81,   test accuracy: 53.59,   loss: 1.43641,   DP costs: [0.696, 1.39, 2.084],   average batch-size: 1002,   best alpha: [23.78, 13.34, 9.58],   time: 135
iteration: 1600,   train accuracy: 54.51,   test accuracy: 54.33,   loss: 1.4583,   DP costs: [0.721, 1.439, 2.157],   average batch-size: 983,   best alpha: [23.13, 12.99, 9.34],   time: 144
iteration: 1700,   train accuracy: 55.03,   test accuracy: 54.01,   loss: 1.55308,   DP costs: [0.745, 1.487, 2.229],   average batch-size: 1044,   best alpha: [22.5, 12.65, 9.1],   time: 152
iteration: 1800,   train accuracy: 54.87,   test accuracy: 55.14,   loss: 1.4602,   DP costs: [0.768, 1.533, 2.298],   average batch-size: 1070,   best alpha: [21.98, 12.36, 8.9],   time: 161
iteration: 1900,   train accuracy: 55.44,   test accuracy: 54.87,   loss: 1.30335,   DP costs: [0.79, 1.578, 2.366],   average batch-size: 1154,   best alpha: [21.46, 12.09, 8.71],   time: 169
iteration: 2000,   train accuracy: 55.91,   test accuracy: 55.84,   loss: 1.47468,   DP costs: [0.813, 1.623, 2.432],   average batch-size: 1029,   best alpha: [20.96, 11.82, 8.55],   time: 177
iteration: 2100,   train accuracy: 56.44,   test accuracy: 56.13,   loss: 1.33495,   DP costs: [0.834, 1.666, 2.497],   average batch-size: 994,   best alpha: [20.48, 11.59, 8.37],   time: 185
iteration: 2200,   train accuracy: 56.65,   test accuracy: 56.11,   loss: 1.37286,   DP costs: [0.855, 1.708, 2.561],   average batch-size: 912,   best alpha: [20.08, 11.38, 8.22],   time: 194
iteration: 2300,   train accuracy: 56.47,   test accuracy: 55.79,   loss: 1.39571,   DP costs: [0.876, 1.75, 2.623],   average batch-size: 1038,   best alpha: [19.69, 11.17, 8.07],   time: 203
iteration: 2400,   train accuracy: 56.11,   test accuracy: 55.89,   loss: 1.34287,   DP costs: [0.896, 1.79, 2.685],   average batch-size: 976,   best alpha: [19.31, 10.96, 7.96],   time: 211
iteration: 2500,   train accuracy: 56.45,   test accuracy: 55.82,   loss: 1.35154,   DP costs: [0.916, 1.83, 2.745],   average batch-size: 1010,   best alpha: [19.01, 10.76, 7.81],   time: 221
iteration: 2600,   train accuracy: 57.25,   test accuracy: 56.13,   loss: 1.33272,   DP costs: [0.936, 1.87, 2.804],   average batch-size: 1027,   best alpha: [18.64, 10.6, 7.7],   time: 230
iteration: 2700,   train accuracy: 58.16,   test accuracy: 56.43,   loss: 1.58377,   DP costs: [0.955, 1.908, 2.862],   average batch-size: 975,   best alpha: [18.35, 10.44, 7.59],   time: 238
iteration: 2800,   train accuracy: 57.28,   test accuracy: 56.39,   loss: 1.39963,   DP costs: [0.974, 1.947, 2.92],   average batch-size: 950,   best alpha: [18.07, 10.28, 7.48],   time: 247
iteration: 2900,   train accuracy: 57.31,   test accuracy: 56.12,   loss: 1.32555,   DP costs: [0.992, 1.984, 2.976],   average batch-size: 902,   best alpha: [17.79, 10.13, 7.38],   time: 255
iteration: 2929,   train accuracy: 57.29,   test accuracy: 56.01,   loss: 1.38914,   DP costs: [0.998, 1.995, 2.992],   average batch-size: 921,   best alpha: [17.72, 10.1, 7.35],   time: 260
Terminate: The maximum of iterations is reached!
