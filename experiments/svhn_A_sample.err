Input args: Namespace(dname='SVHN', seeds=[0], architecture='CIFAR10_CNN', accountant='rdp', individualize='sampling', log_iteration=100, lr=0.2, momentum=0.5, epochs=80, n_workers=6, batch_size=1024, max_physical_batch_size=1024, delta=1e-05, budgets=[1.0, 2.0, 3.0], ratios=[0.34, 0.43, 0.23], max_grad_norm=0.6, noise_multiplier=2.53261, weights=None, adapt_weights_to_budgets=True, use_cuda='True', save_path='../svhn_results/sampling/SVHN/epochs_80_batch_1024_lr_0.2_max_grad_norm_0.6_budgets_1.0_2.0_3.0_ratios_0.34_0.43_0.23_seeds_0', mode='mia', accuracy_log='accuracy.log', assign_budget='even', class_budgets=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], mia_ndata=73257, mia_count=0, allow_excess=False, save_model='True')
  0%|          | 0/182040794 [00:00<?, ?it/s]  0%|          | 32768/182040794 [00:00<10:44, 282595.01it/s]  0%|          | 98304/182040794 [00:00<08:52, 341701.51it/s]  0%|          | 163840/182040794 [00:00<07:07, 425032.48it/s]  0%|          | 262144/182040794 [00:00<05:18, 571399.22it/s]  0%|          | 360448/182040794 [00:00<04:35, 659452.09it/s]  0%|          | 524288/182040794 [00:00<03:22, 895722.82it/s]  0%|          | 688128/182040794 [00:00<02:52, 1049872.00it/s]  0%|          | 884736/182040794 [00:01<02:26, 1238653.12it/s]  1%|          | 1114112/182040794 [00:01<02:04, 1452964.32it/s]  1%|          | 1376256/182040794 [00:01<01:47, 1682676.72it/s]  1%|          | 1671168/182040794 [00:01<01:33, 1926749.69it/s]  1%|          | 2031616/182040794 [00:01<01:19, 2262337.62it/s]  1%|▏         | 2424832/182040794 [00:01<01:09, 2580488.71it/s]  2%|▏         | 2850816/182040794 [00:01<01:02, 2881280.30it/s]  2%|▏         | 3407872/182040794 [00:01<00:52, 3422113.85it/s]  2%|▏         | 4030464/182040794 [00:01<00:44, 3971985.39it/s]  3%|▎         | 4751360/182040794 [00:02<00:38, 4576394.36it/s]  3%|▎         | 5570560/182040794 [00:02<00:33, 5238526.49it/s]  3%|▎         | 6291456/182040794 [00:02<00:31, 5519389.10it/s]  4%|▍         | 7241728/182040794 [00:02<00:27, 6414325.96it/s]  5%|▍         | 8323072/182040794 [00:02<00:24, 7072537.12it/s]  5%|▌         | 9535488/182040794 [00:02<00:21, 8204253.93it/s]  6%|▌         | 11108352/182040794 [00:02<00:17, 9511538.57it/s]  7%|▋         | 12058624/182040794 [00:05<02:15, 1257594.79it/s]  7%|▋         | 12746752/182040794 [00:12<08:25, 335119.55it/s]   7%|▋         | 13238272/182040794 [00:14<08:13, 341817.03it/s]  7%|▋         | 13598720/182040794 [00:14<07:21, 381773.07it/s]  8%|▊         | 13893632/182040794 [00:14<06:32, 427958.12it/s]  8%|▊         | 14123008/182040794 [00:14<05:47, 483555.41it/s]  8%|▊         | 14352384/182040794 [00:15<05:01, 556026.04it/s]  8%|▊         | 14581760/182040794 [00:15<04:18, 647828.16it/s]  8%|▊         | 14876672/182040794 [00:15<03:26, 807658.59it/s]  8%|▊         | 15204352/182040794 [00:15<02:42, 1024761.01it/s]  9%|▊         | 15597568/182040794 [00:15<02:04, 1337822.16it/s]  9%|▉         | 16056320/182040794 [00:15<01:34, 1748780.13it/s]  9%|▉         | 16547840/182040794 [00:15<01:15, 2205669.55it/s]  9%|▉         | 17137664/182040794 [00:15<00:59, 2792433.63it/s] 10%|▉         | 17825792/182040794 [00:16<00:47, 3491000.36it/s] 10%|█         | 18579456/182040794 [00:16<00:38, 4204125.91it/s] 11%|█         | 19431424/182040794 [00:16<00:32, 4985977.34it/s] 11%|█         | 20381696/182040794 [00:16<00:27, 5807523.76it/s] 12%|█▏        | 21430272/182040794 [00:16<00:24, 6655008.92it/s] 12%|█▏        | 22609920/182040794 [00:16<00:21, 7567414.87it/s] 13%|█▎        | 23887872/182040794 [00:16<00:18, 8581742.39it/s] 14%|█▍        | 25100288/182040794 [00:16<00:17, 9068521.44it/s] 15%|█▍        | 26443776/182040794 [00:17<00:15, 9734362.06it/s] 15%|█▌        | 27787264/182040794 [00:17<00:15, 10207752.47it/s] 16%|█▌        | 29163520/182040794 [00:17<00:14, 10608180.21it/s] 17%|█▋        | 30507008/182040794 [00:17<00:13, 10847594.89it/s] 18%|█▊        | 31883264/182040794 [00:17<00:13, 11024960.29it/s] 18%|█▊        | 33226752/182040794 [00:17<00:13, 11138291.43it/s] 19%|█▉        | 34603008/182040794 [00:17<00:13, 11117646.53it/s] 20%|█▉        | 35946496/182040794 [00:17<00:12, 11353617.86it/s] 20%|██        | 37289984/182040794 [00:17<00:12, 11364672.95it/s] 21%|██        | 38436864/182040794 [00:24<03:55, 610250.45it/s]   22%|██▏       | 39256064/182040794 [00:27<05:01, 472984.38it/s] 22%|██▏       | 39845888/182040794 [00:28<04:23, 539462.61it/s] 22%|██▏       | 40304640/182040794 [00:28<03:51, 613457.88it/s] 22%|██▏       | 40697856/182040794 [00:28<03:19, 707315.21it/s] 23%|██▎       | 41058304/182040794 [00:28<02:51, 821179.74it/s] 23%|██▎       | 41385984/182040794 [00:28<02:28, 947819.57it/s] 23%|██▎       | 41779200/182040794 [00:28<02:01, 1153394.27it/s] 23%|██▎       | 42303488/182040794 [00:29<01:32, 1509778.75it/s] 24%|██▎       | 42893312/182040794 [00:29<01:10, 1976373.62it/s] 24%|██▍       | 43548672/182040794 [00:29<00:54, 2546261.89it/s] 24%|██▍       | 44335104/182040794 [00:29<00:41, 3309187.50it/s] 25%|██▍       | 45187072/182040794 [00:29<00:33, 4125821.04it/s] 25%|██▌       | 46137344/182040794 [00:29<00:27, 5020458.25it/s] 26%|██▌       | 47153152/182040794 [00:29<00:21, 6138713.40it/s] 26%|██▋       | 48005120/182040794 [00:29<00:20, 6475308.94it/s] 27%|██▋       | 49119232/182040794 [00:30<00:18, 7310512.76it/s] 28%|██▊       | 50331648/182040794 [00:30<00:16, 8140187.33it/s] 28%|██▊       | 51609600/182040794 [00:30<00:14, 8890790.47it/s] 29%|██▉       | 52953088/182040794 [00:30<00:13, 9611842.04it/s] 30%|██▉       | 54362112/182040794 [00:30<00:12, 10265527.99it/s] 31%|███       | 55836672/182040794 [00:30<00:11, 10922098.68it/s] 32%|███▏      | 57442304/182040794 [00:30<00:10, 11694057.97it/s] 32%|███▏      | 59113472/182040794 [00:30<00:09, 12408514.95it/s] 33%|███▎      | 60915712/182040794 [00:30<00:09, 13234398.41it/s] 35%|███▍      | 62816256/182040794 [00:31<00:08, 14058369.22it/s] 36%|███▌      | 64847872/182040794 [00:31<00:07, 14981502.39it/s] 37%|███▋      | 67010560/182040794 [00:31<00:07, 15956204.12it/s] 38%|███▊      | 69304320/182040794 [00:31<00:06, 16963092.33it/s] 39%|███▉      | 71761920/182040794 [00:31<00:06, 18034780.95it/s] 41%|████      | 74350592/182040794 [00:31<00:05, 19214558.77it/s] 42%|████▏     | 76840960/182040794 [00:31<00:05, 20676263.79it/s] 43%|████▎     | 78938112/182040794 [00:31<00:05, 20544282.88it/s] 45%|████▍     | 81723392/182040794 [00:32<00:04, 21866178.07it/s] 46%|████▋     | 84541440/182040794 [00:32<00:04, 23559565.88it/s] 48%|████▊     | 87031808/182040794 [00:32<00:03, 23762343.33it/s] 49%|████▉     | 89456640/182040794 [00:32<00:03, 23846684.71it/s] 51%|█████     | 92438528/182040794 [00:32<00:03, 24774201.97it/s] 52%|█████▏    | 95420416/182040794 [00:32<00:03, 26032337.42it/s] 54%|█████▍    | 98041856/182040794 [00:32<00:03, 24927731.10it/s] 55%|█████▌    | 100892672/182040794 [00:32<00:03, 25922006.47it/s] 57%|█████▋    | 103514112/182040794 [00:32<00:03, 25234532.44it/s] 58%|█████▊    | 106397696/182040794 [00:32<00:02, 25885915.59it/s] 60%|█████▉    | 109019136/182040794 [00:33<00:02, 25336723.45it/s] 61%|██████▏   | 111935488/182040794 [00:33<00:02, 26007777.09it/s] 63%|██████▎   | 114556928/182040794 [00:33<00:02, 25559149.00it/s] 64%|██████▍   | 117374976/182040794 [00:33<00:02, 26037475.62it/s] 66%|██████▌   | 119996416/182040794 [00:33<00:02, 25475706.12it/s] 67%|██████▋   | 122814464/182040794 [00:33<00:02, 26163397.28it/s] 69%|██████▉   | 125468672/182040794 [00:33<00:02, 25679199.34it/s] 70%|███████   | 128286720/182040794 [00:33<00:02, 26356287.96it/s] 72%|███████▏  | 130940928/182040794 [00:33<00:01, 25699664.98it/s] 73%|███████▎  | 133726208/182040794 [00:34<00:01, 26140970.74it/s] 75%|███████▍  | 136347648/182040794 [00:34<00:01, 25837341.56it/s] 76%|███████▋  | 138936320/182040794 [00:34<00:01, 25602356.05it/s] 78%|███████▊  | 141721600/182040794 [00:34<00:01, 26050535.10it/s] 79%|███████▉  | 144375808/182040794 [00:34<00:01, 26131838.69it/s] 81%|████████  | 146997248/182040794 [00:34<00:01, 26085411.86it/s] 82%|████████▏ | 149618688/182040794 [00:34<00:01, 25860122.12it/s] 84%|████████▎ | 152436736/182040794 [00:34<00:01, 26531663.14it/s] 85%|████████▌ | 155123712/182040794 [00:34<00:01, 26252251.99it/s] 87%|████████▋ | 157777920/182040794 [00:34<00:00, 25321915.53it/s] 88%|████████▊ | 160628736/182040794 [00:35<00:00, 26112092.07it/s] 90%|████████▉ | 163414016/182040794 [00:35<00:00, 26540665.17it/s] 91%|█████████ | 166100992/182040794 [00:35<00:00, 26373668.23it/s] 93%|█████████▎| 168755200/182040794 [00:36<00:01, 7963408.77it/s]  94%|█████████▍| 170721280/182040794 [00:36<00:01, 8458848.96it/s] 95%|█████████▍| 172425216/182040794 [00:36<00:01, 9369588.36it/s] 96%|█████████▌| 174522368/182040794 [00:36<00:00, 10933738.01it/s] 97%|█████████▋| 176717824/182040794 [00:36<00:00, 12467852.55it/s] 98%|█████████▊| 178946048/182040794 [00:36<00:00, 13863981.79it/s]100%|█████████▉| 181174272/182040794 [00:36<00:00, 15055808.57it/s]100%|██████████| 182040794/182040794 [00:36<00:00, 4926746.20it/s] 
  0%|          | 0/64275384 [00:00<?, ?it/s]  0%|          | 32768/64275384 [00:00<03:54, 273611.63it/s]  0%|          | 98304/64275384 [00:00<03:12, 334217.39it/s]  0%|          | 163840/64275384 [00:00<02:34, 413829.98it/s]  0%|          | 229376/64275384 [00:00<02:19, 460131.61it/s]  1%|          | 327680/64275384 [00:00<01:50, 579969.24it/s]  1%|          | 425984/64275384 [00:00<01:37, 656527.83it/s]  1%|          | 557056/64275384 [00:00<01:20, 792404.63it/s]  1%|          | 688128/64275384 [00:01<01:11, 884874.07it/s]  1%|▏         | 851968/64275384 [00:01<01:01, 1029875.15it/s]  2%|▏         | 1015808/64275384 [00:01<00:55, 1129658.85it/s]  2%|▏         | 1212416/64275384 [00:01<00:49, 1281839.82it/s]  2%|▏         | 1474560/64275384 [00:01<00:40, 1549614.72it/s]  3%|▎         | 1736704/64275384 [00:01<00:36, 1736585.01it/s]  3%|▎         | 2031616/64275384 [00:01<00:31, 1948862.86it/s]  4%|▎         | 2392064/64275384 [00:01<00:27, 2257561.73it/s]  4%|▍         | 2785280/64275384 [00:01<00:24, 2556640.52it/s]  5%|▌         | 3244032/64275384 [00:02<00:20, 2917946.19it/s]  6%|▌         | 3768320/64275384 [00:02<00:18, 3343885.49it/s]  7%|▋         | 4325376/64275384 [00:02<00:16, 3717623.66it/s]  8%|▊         | 4915200/64275384 [00:02<00:14, 4067152.38it/s]  9%|▊         | 5570560/64275384 [00:02<00:13, 4473629.15it/s] 10%|▉         | 6324224/64275384 [00:02<00:11, 5006509.47it/s] 11%|█         | 7143424/64275384 [00:02<00:10, 5533829.16it/s] 13%|█▎        | 8060928/64275384 [00:02<00:09, 6155927.16it/s] 14%|█▍        | 9142272/64275384 [00:03<00:07, 6997973.38it/s] 16%|█▌        | 10289152/64275384 [00:03<00:06, 7748662.79it/s] 18%|█▊        | 11567104/64275384 [00:03<00:06, 8602949.46it/s] 20%|██        | 12943360/64275384 [00:03<00:05, 9441862.75it/s] 23%|██▎       | 14483456/64275384 [00:03<00:04, 10427918.39it/s] 25%|██▌       | 16121856/64275384 [00:03<00:04, 11333073.40it/s] 28%|██▊       | 17924096/64275384 [00:03<00:03, 12404644.62it/s] 31%|███       | 19890176/64275384 [00:03<00:03, 13560879.31it/s] 34%|███▍      | 22052864/64275384 [00:04<00:02, 14878409.18it/s] 37%|███▋      | 23560192/64275384 [00:04<00:02, 14864721.83it/s] 40%|███▉      | 25591808/64275384 [00:04<00:02, 16042821.27it/s] 43%|████▎     | 27885568/64275384 [00:04<00:02, 17754901.64it/s] 46%|████▌     | 29687808/64275384 [00:04<00:01, 17417410.03it/s] 49%|████▉     | 31784960/64275384 [00:04<00:01, 17448482.61it/s] 54%|█████▎    | 34439168/64275384 [00:04<00:01, 18870886.65it/s] 58%|█████▊    | 37191680/64275384 [00:04<00:01, 20075748.01it/s] 62%|██████▏   | 40042496/64275384 [00:04<00:01, 21160011.48it/s] 66%|██████▌   | 42401792/64275384 [00:05<00:01, 20615304.08it/s] 70%|██████▉   | 44859392/64275384 [00:05<00:00, 20534517.76it/s] 73%|███████▎  | 46923776/64275384 [00:05<00:00, 20444150.67it/s] 76%|███████▋  | 49053696/64275384 [00:05<00:00, 20481381.63it/s] 80%|████████  | 51675136/64275384 [00:05<00:00, 22062790.00it/s] 84%|████████▍ | 53903360/64275384 [00:05<00:00, 21289626.61it/s] 87%|████████▋ | 56229888/64275384 [00:05<00:00, 21363592.19it/s] 91%|█████████ | 58392576/64275384 [00:12<00:05, 1050444.78it/s]  93%|█████████▎| 59932672/64275384 [00:13<00:03, 1250061.78it/s] 95%|█████████▌| 61112320/64275384 [00:13<00:02, 1483059.33it/s] 97%|█████████▋| 62128128/64275384 [00:13<00:01, 1737711.96it/s] 98%|█████████▊| 63045632/64275384 [00:13<00:00, 2058672.65it/s]100%|█████████▉| 64159744/64275384 [00:13<00:00, 2575486.04it/s]100%|██████████| 64275384/64275384 [00:13<00:00, 4678138.06it/s]
seed: 0,   max_iteration: 5723,   1 epoch ~= 72 iterations
seed: 0,   max_iteration: 5723,   1 epoch ~= 72 iterations
/local/scratch/manwa22/idp-sgd/experiments/../opacus/opacus/privacy_engine.py:126: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/local/scratch/manwa22/idp-sgd/experiments/../opacus/opacus/accountants/analysis/rdp.py:74: RuntimeWarning: invalid value encountered in scalar subtract
  return math.log1p(math.exp(a - b)) + b  # log1p(x) = log(x + 1)
Initializing privacy parameters:   max_grad_norm=0.6,   sample_rate=0.013888888888888888,   noise_multiplier 2.4178713378906256,   individual sample_rates=[0.007446289988037109, 0.014025879765991213, 0.020190430485595705]
/local/scratch/manwa22/miniconda3/envs/idp/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
iteration: 0,   train accuracy: 10.71,   test accuracy: 10.06,   loss: 2.29948,   DP costs: [0.115, 0.137, 0.154],   average batch-size: 921,   best alpha: [57.52, 49.94, 45.61],   time: 1
iteration: 100,   train accuracy: 18.89,   test accuracy: 19.59,   loss: 2.25377,   DP costs: [0.15, 0.244, 0.358],   average batch-size: 1010,   best alpha: [56.37, 48.54, 38.82],   time: 11
iteration: 200,   train accuracy: 18.87,   test accuracy: 19.59,   loss: 2.22888,   DP costs: [0.182, 0.341, 0.505],   average batch-size: 1007,   best alpha: [56.14, 41.48, 29.69],   time: 22
iteration: 300,   train accuracy: 19.11,   test accuracy: 19.59,   loss: 2.23788,   DP costs: [0.213, 0.418, 0.62],   average batch-size: 1105,   best alpha: [55.91, 35.18, 25.23],   time: 33
iteration: 400,   train accuracy: 18.9,   test accuracy: 19.59,   loss: 2.21043,   DP costs: [0.245, 0.485, 0.719],   average batch-size: 1042,   best alpha: [55.46, 31.33, 22.41],   time: 44
iteration: 500,   train accuracy: 18.41,   test accuracy: 19.59,   loss: 2.26032,   DP costs: [0.275, 0.544, 0.808],   average batch-size: 943,   best alpha: [50.94, 28.47, 20.47],   time: 55
iteration: 600,   train accuracy: 19.06,   test accuracy: 19.59,   loss: 2.2265,   DP costs: [0.302, 0.598, 0.888],   average batch-size: 978,   best alpha: [47.18, 26.4, 19.0],   time: 66
iteration: 700,   train accuracy: 18.77,   test accuracy: 19.59,   loss: 2.27266,   DP costs: [0.327, 0.649, 0.963],   average batch-size: 926,   best alpha: [44.23, 24.68, 17.79],   time: 76
iteration: 800,   train accuracy: 18.86,   test accuracy: 19.59,   loss: 2.25825,   DP costs: [0.351, 0.695, 1.033],   average batch-size: 1062,   best alpha: [41.64, 23.35, 16.84],   time: 87
iteration: 900,   train accuracy: 18.84,   test accuracy: 19.59,   loss: 2.23893,   DP costs: [0.373, 0.74, 1.099],   average batch-size: 973,   best alpha: [39.68, 22.19, 16.02],   time: 98
iteration: 1000,   train accuracy: 18.94,   test accuracy: 19.59,   loss: 2.22356,   DP costs: [0.394, 0.782, 1.162],   average batch-size: 1025,   best alpha: [37.81, 21.25, 15.35],   time: 107
iteration: 1100,   train accuracy: 18.79,   test accuracy: 19.59,   loss: 2.22726,   DP costs: [0.414, 0.822, 1.222],   average batch-size: 1068,   best alpha: [36.32, 20.43, 14.71],   time: 118
iteration: 1200,   train accuracy: 19.06,   test accuracy: 19.59,   loss: 2.25531,   DP costs: [0.434, 0.861, 1.28],   average batch-size: 1031,   best alpha: [35.03, 19.64, 14.21],   time: 129
iteration: 1300,   train accuracy: 18.99,   test accuracy: 19.59,   loss: 2.24537,   DP costs: [0.452, 0.898, 1.336],   average batch-size: 1009,   best alpha: [33.79, 19.04, 13.73],   time: 139
iteration: 1400,   train accuracy: 18.99,   test accuracy: 19.59,   loss: 2.2298,   DP costs: [0.47, 0.934, 1.389],   average batch-size: 1016,   best alpha: [32.73, 18.38, 13.32],   time: 150
iteration: 1500,   train accuracy: 18.95,   test accuracy: 19.59,   loss: 2.26253,   DP costs: [0.488, 0.969, 1.442],   average batch-size: 1060,   best alpha: [31.7, 17.89, 12.92],   time: 160
iteration: 1600,   train accuracy: 18.99,   test accuracy: 19.59,   loss: 2.25064,   DP costs: [0.505, 1.003, 1.492],   average batch-size: 992,   best alpha: [30.83, 17.41, 12.58],   time: 171
iteration: 1700,   train accuracy: 18.95,   test accuracy: 19.59,   loss: 2.20525,   DP costs: [0.521, 1.036, 1.542],   average batch-size: 915,   best alpha: [30.1, 16.94, 12.3],   time: 181
iteration: 1800,   train accuracy: 18.76,   test accuracy: 19.59,   loss: 2.26515,   DP costs: [0.537, 1.068, 1.59],   average batch-size: 998,   best alpha: [29.27, 16.55, 11.98],   time: 191
iteration: 1900,   train accuracy: 19.11,   test accuracy: 19.59,   loss: 2.23961,   DP costs: [0.553, 1.099, 1.636],   average batch-size: 945,   best alpha: [28.69, 16.17, 11.71],   time: 202
iteration: 2000,   train accuracy: 18.9,   test accuracy: 19.6,   loss: 2.24911,   DP costs: [0.568, 1.129, 1.682],   average batch-size: 999,   best alpha: [28.02, 15.8, 11.49],   time: 212
iteration: 2100,   train accuracy: 18.94,   test accuracy: 19.6,   loss: 2.23913,   DP costs: [0.583, 1.159, 1.727],   average batch-size: 1131,   best alpha: [27.47, 15.5, 11.23],   time: 223
iteration: 2200,   train accuracy: 19.2,   test accuracy: 19.59,   loss: 2.23409,   DP costs: [0.598, 1.188, 1.771],   average batch-size: 1098,   best alpha: [26.82, 15.2, 11.02],   time: 234
iteration: 2300,   train accuracy: 18.94,   test accuracy: 19.59,   loss: 2.23074,   DP costs: [0.612, 1.217, 1.814],   average batch-size: 1057,   best alpha: [26.4, 14.91, 10.82],   time: 244
iteration: 2400,   train accuracy: 18.91,   test accuracy: 19.56,   loss: 2.25626,   DP costs: [0.626, 1.245, 1.856],   average batch-size: 959,   best alpha: [25.88, 14.63, 10.62],   time: 253
iteration: 2500,   train accuracy: 19.1,   test accuracy: 19.65,   loss: 2.24628,   DP costs: [0.64, 1.273, 1.898],   average batch-size: 990,   best alpha: [25.37, 14.41, 10.46],   time: 262
iteration: 2600,   train accuracy: 19.2,   test accuracy: 19.63,   loss: 2.26069,   DP costs: [0.653, 1.3, 1.938],   average batch-size: 941,   best alpha: [24.97, 14.13, 10.31],   time: 272
iteration: 2700,   train accuracy: 19.16,   test accuracy: 19.72,   loss: 2.21079,   DP costs: [0.666, 1.326, 1.979],   average batch-size: 991,   best alpha: [24.58, 13.92, 10.12],   time: 283
iteration: 2800,   train accuracy: 18.76,   test accuracy: 19.66,   loss: 2.26099,   DP costs: [0.679, 1.352, 2.018],   average batch-size: 1011,   best alpha: [24.19, 13.71, 9.97],   time: 293
iteration: 2900,   train accuracy: 18.92,   test accuracy: 19.74,   loss: 2.22489,   DP costs: [0.692, 1.378, 2.057],   average batch-size: 1143,   best alpha: [23.81, 13.5, 9.82],   time: 305
iteration: 3000,   train accuracy: 18.67,   test accuracy: 19.79,   loss: 2.24484,   DP costs: [0.705, 1.404, 2.095],   average batch-size: 1010,   best alpha: [23.44, 13.29, 9.71],   time: 315
iteration: 3100,   train accuracy: 18.47,   test accuracy: 19.0,   loss: 2.2667,   DP costs: [0.717, 1.429, 2.133],   average batch-size: 1022,   best alpha: [23.07, 13.14, 9.57],   time: 325
iteration: 3200,   train accuracy: 18.95,   test accuracy: 19.56,   loss: 2.22428,   DP costs: [0.729, 1.453, 2.17],   average batch-size: 1048,   best alpha: [22.8, 12.94, 9.43],   time: 336
iteration: 3300,   train accuracy: 18.71,   test accuracy: 19.76,   loss: 2.22982,   DP costs: [0.741, 1.478, 2.207],   average batch-size: 1047,   best alpha: [22.53, 12.8, 9.33],   time: 347
iteration: 3400,   train accuracy: 18.55,   test accuracy: 19.24,   loss: 2.24459,   DP costs: [0.753, 1.502, 2.243],   average batch-size: 965,   best alpha: [22.18, 12.6, 9.22],   time: 357
iteration: 3500,   train accuracy: 18.67,   test accuracy: 19.68,   loss: 2.24183,   DP costs: [0.765, 1.525, 2.279],   average batch-size: 986,   best alpha: [21.92, 12.46, 9.09],   time: 367
iteration: 3600,   train accuracy: 18.97,   test accuracy: 20.0,   loss: 2.24279,   DP costs: [0.777, 1.549, 2.314],   average batch-size: 990,   best alpha: [21.66, 12.32, 8.99],   time: 376
iteration: 3700,   train accuracy: 18.97,   test accuracy: 19.63,   loss: 2.2097,   DP costs: [0.788, 1.572, 2.349],   average batch-size: 1043,   best alpha: [21.41, 12.18, 8.89],   time: 386
iteration: 3800,   train accuracy: 18.62,   test accuracy: 18.04,   loss: 2.22562,   DP costs: [0.799, 1.595, 2.384],   average batch-size: 1020,   best alpha: [21.16, 12.04, 8.8],   time: 397
iteration: 3900,   train accuracy: 19.26,   test accuracy: 19.06,   loss: 2.22539,   DP costs: [0.811, 1.617, 2.418],   average batch-size: 1006,   best alpha: [20.91, 11.91, 8.7],   time: 407
iteration: 4000,   train accuracy: 18.71,   test accuracy: 17.85,   loss: 2.22212,   DP costs: [0.822, 1.64, 2.452],   average batch-size: 1026,   best alpha: [20.67, 11.77, 8.6],   time: 417
iteration: 4100,   train accuracy: 18.55,   test accuracy: 18.47,   loss: 2.23529,   DP costs: [0.833, 1.662, 2.485],   average batch-size: 1041,   best alpha: [20.42, 11.64, 8.54],   time: 428
iteration: 4200,   train accuracy: 18.56,   test accuracy: 19.16,   loss: 2.25231,   DP costs: [0.843, 1.683, 2.518],   average batch-size: 985,   best alpha: [20.27, 11.55, 8.45],   time: 439
iteration: 4300,   train accuracy: 18.61,   test accuracy: 18.81,   loss: 2.20043,   DP costs: [0.854, 1.705, 2.551],   average batch-size: 1050,   best alpha: [20.03, 11.42, 8.36],   time: 449
iteration: 4400,   train accuracy: 18.33,   test accuracy: 17.85,   loss: 2.22685,   DP costs: [0.865, 1.726, 2.583],   average batch-size: 1066,   best alpha: [19.79, 11.3, 8.3],   time: 460
iteration: 4500,   train accuracy: 18.73,   test accuracy: 17.43,   loss: 2.23609,   DP costs: [0.875, 1.748, 2.616],   average batch-size: 1035,   best alpha: [19.64, 11.21, 8.21],   time: 470
iteration: 4600,   train accuracy: 18.25,   test accuracy: 17.48,   loss: 2.2512,   DP costs: [0.886, 1.769, 2.647],   average batch-size: 1064,   best alpha: [19.49, 11.09, 8.15],   time: 480
iteration: 4700,   train accuracy: 18.53,   test accuracy: 18.36,   loss: 2.20815,   DP costs: [0.896, 1.789, 2.679],   average batch-size: 1040,   best alpha: [19.26, 11.0, 8.06],   time: 490
iteration: 4800,   train accuracy: 18.26,   test accuracy: 18.58,   loss: 2.21246,   DP costs: [0.906, 1.81, 2.71],   average batch-size: 1026,   best alpha: [19.11, 10.92, 8.0],   time: 501
iteration: 4900,   train accuracy: 18.9,   test accuracy: 18.62,   loss: 2.23308,   DP costs: [0.916, 1.83, 2.741],   average batch-size: 995,   best alpha: [18.89, 10.8, 7.92],   time: 512
iteration: 5000,   train accuracy: 18.89,   test accuracy: 18.73,   loss: 2.23241,   DP costs: [0.926, 1.851, 2.772],   average batch-size: 1012,   best alpha: [18.74, 10.72, 7.86],   time: 522
iteration: 5100,   train accuracy: 18.55,   test accuracy: 18.04,   loss: 2.23502,   DP costs: [0.936, 1.871, 2.802],   average batch-size: 1030,   best alpha: [18.59, 10.64, 7.8],   time: 532
iteration: 5200,   train accuracy: 18.75,   test accuracy: 18.42,   loss: 2.23269,   DP costs: [0.946, 1.891, 2.833],   average batch-size: 1022,   best alpha: [18.45, 10.56, 7.75],   time: 543
iteration: 5300,   train accuracy: 18.69,   test accuracy: 17.96,   loss: 2.23053,   DP costs: [0.955, 1.91, 2.863],   average batch-size: 1102,   best alpha: [18.31, 10.48, 7.69],   time: 553
iteration: 5400,   train accuracy: 18.97,   test accuracy: 18.2,   loss: 2.24623,   DP costs: [0.965, 1.93, 2.892],   average batch-size: 1103,   best alpha: [18.16, 10.36, 7.64],   time: 564
iteration: 5500,   train accuracy: 18.69,   test accuracy: 18.7,   loss: 2.25696,   DP costs: [0.975, 1.949, 2.922],   average batch-size: 1032,   best alpha: [18.02, 10.29, 7.56],   time: 575
iteration: 5600,   train accuracy: 18.78,   test accuracy: 18.61,   loss: 2.20443,   DP costs: [0.984, 1.969, 2.951],   average batch-size: 1030,   best alpha: [17.81, 10.21, 7.5],   time: 586
iteration: 5700,   train accuracy: 19.02,   test accuracy: 18.87,   loss: 2.21608,   DP costs: [0.994, 1.988, 2.98],   average batch-size: 1009,   best alpha: [17.68, 10.14, 7.45],   time: 596
iteration: 5722,   train accuracy: 17.91,   test accuracy: 17.94,   loss: 2.26612,   DP costs: [0.996, 1.992, 2.987],   average batch-size: 958,   best alpha: [17.68, 10.14, 7.45],   time: 603
Terminate: The maximum of iterations is reached!
