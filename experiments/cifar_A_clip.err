Input args: Namespace(dname='CIFAR10', seeds=[0], architecture='CIFAR10_CNN', accountant='rdp', individualize='clipping', log_iteration=100, lr=0.2, momentum=0.5, epochs=70, n_workers=6, batch_size=1024, max_physical_batch_size=128, delta=1e-05, budgets=[1.0, 2.0, 3.0], ratios=[0.34, 0.43, 0.23], max_grad_norm=1.1, noise_multiplier=2.88335, weights=None, adapt_weights_to_budgets=True, use_cuda='True', save_path='../cifar_results/clipping/CIFAR10/epochs_70_batch_1024_lr_0.2_max_grad_norm_1.1_budgets_1.0_2.0_3.0_ratios_0.34_0.43_0.23_seeds_0', mode='mia', accuracy_log='accuracy.log', assign_budget='even', class_budgets=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], mia_ndata=50000, mia_count=0, allow_excess=False, save_model='True')
  0%|          | 0/170498071 [00:00<?, ?it/s]  0%|          | 98304/170498071 [00:00<02:53, 980325.02it/s]  1%|          | 917504/170498071 [00:00<00:32, 5153691.66it/s]  3%|▎         | 5734400/170498071 [00:00<00:06, 24448890.14it/s]  7%|▋         | 11599872/170498071 [00:00<00:04, 37525651.58it/s] 10%|█         | 17334272/170498071 [00:00<00:03, 39187897.40it/s] 14%|█▎        | 23396352/170498071 [00:00<00:03, 45505129.42it/s] 16%|█▋        | 27983872/170498071 [00:00<00:03, 44695775.06it/s] 19%|█▉        | 32473088/170498071 [00:00<00:03, 44324330.71it/s] 22%|██▏       | 36929536/170498071 [00:00<00:03, 44089809.96it/s] 24%|██▍       | 41353216/170498071 [00:01<00:02, 44094538.38it/s] 27%|██▋       | 45776896/170498071 [00:01<00:02, 44088027.45it/s] 29%|██▉       | 50233344/170498071 [00:01<00:02, 44138111.89it/s] 32%|███▏      | 54689792/170498071 [00:01<00:02, 44178793.99it/s] 35%|███▍      | 59244544/170498071 [00:01<00:02, 44506333.42it/s] 37%|███▋      | 63700992/170498071 [00:01<00:02, 39775680.70it/s] 40%|████      | 68517888/170498071 [00:01<00:02, 41981437.90it/s] 43%|████▎     | 72810496/170498071 [00:01<00:02, 39283166.88it/s] 45%|████▌     | 76840960/170498071 [00:01<00:02, 37042729.78it/s] 47%|████▋     | 80642048/170498071 [00:02<00:02, 35856537.35it/s] 49%|████▉     | 84279296/170498071 [00:02<00:02, 31714991.53it/s] 51%|█████▏    | 87556096/170498071 [00:02<00:03, 23415156.68it/s] 53%|█████▎    | 90275840/170498071 [00:02<00:04, 19194402.38it/s] 54%|█████▍    | 92536832/170498071 [00:02<00:04, 16946263.05it/s] 55%|█████▌    | 94470144/170498071 [00:03<00:04, 15708586.93it/s] 56%|█████▋    | 96206848/170498071 [00:03<00:05, 14805030.28it/s] 57%|█████▋    | 97779712/170498071 [00:03<00:05, 14249549.10it/s] 58%|█████▊    | 99287040/170498071 [00:03<00:05, 13747205.14it/s] 59%|█████▉    | 100696064/170498071 [00:03<00:05, 13413865.00it/s] 60%|█████▉    | 102072320/170498071 [00:03<00:05, 13187248.38it/s] 61%|██████    | 103415808/170498071 [00:03<00:05, 13114470.10it/s] 61%|██████▏   | 104759296/170498071 [00:03<00:05, 13007983.43it/s] 62%|██████▏   | 106070016/170498071 [00:03<00:05, 12841972.89it/s] 63%|██████▎   | 107380736/170498071 [00:04<00:04, 12856854.25it/s] 64%|██████▎   | 108691456/170498071 [00:04<00:04, 12913423.65it/s] 65%|██████▍   | 110002176/170498071 [00:04<00:04, 12857034.19it/s] 65%|██████▌   | 111345664/170498071 [00:04<00:04, 12900227.38it/s] 66%|██████▌   | 112721920/170498071 [00:04<00:04, 13027402.89it/s] 67%|██████▋   | 114032640/170498071 [00:04<00:04, 12463394.74it/s] 68%|██████▊   | 115310592/170498071 [00:04<00:04, 12362313.25it/s] 68%|██████▊   | 116555776/170498071 [00:04<00:05, 10647439.60it/s] 69%|██████▉   | 117669888/170498071 [00:05<00:05, 9244455.11it/s]  70%|██████▉   | 118652928/170498071 [00:05<00:06, 8580077.60it/s] 70%|███████   | 119570432/170498071 [00:05<00:06, 8267716.75it/s] 71%|███████   | 120422400/170498071 [00:05<00:06, 7898376.44it/s] 71%|███████   | 121241600/170498071 [00:05<00:06, 7846319.18it/s] 72%|███████▏  | 122060800/170498071 [00:05<00:06, 7670674.43it/s] 72%|███████▏  | 122847232/170498071 [00:05<00:06, 7392936.22it/s] 72%|███████▏  | 123600896/170498071 [00:05<00:07, 6688094.22it/s] 73%|███████▎  | 124289024/170498071 [00:06<00:07, 6361268.29it/s] 73%|███████▎  | 124944384/170498071 [00:06<00:07, 6139280.47it/s] 74%|███████▎  | 125566976/170498071 [00:06<00:07, 5969498.87it/s] 74%|███████▍  | 126189568/170498071 [00:06<00:07, 5908792.96it/s] 74%|███████▍  | 126812160/170498071 [00:06<00:07, 5840651.75it/s] 75%|███████▍  | 127401984/170498071 [00:06<00:07, 5830430.04it/s] 75%|███████▌  | 127991808/170498071 [00:06<00:07, 5821580.61it/s] 75%|███████▌  | 128614400/170498071 [00:06<00:07, 5890758.33it/s] 76%|███████▌  | 129204224/170498071 [00:06<00:07, 5866971.81it/s] 76%|███████▌  | 129826816/170498071 [00:06<00:06, 5932246.68it/s] 77%|███████▋  | 130449408/170498071 [00:07<00:06, 5979127.95it/s] 77%|███████▋  | 131072000/170498071 [00:07<00:06, 6018370.96it/s] 77%|███████▋  | 131694592/170498071 [00:07<00:06, 5673416.78it/s] 78%|███████▊  | 132284416/170498071 [00:07<00:07, 5218484.17it/s] 78%|███████▊  | 132841472/170498071 [00:07<00:07, 5014759.56it/s] 78%|███████▊  | 133365760/170498071 [00:07<00:07, 4868334.18it/s] 79%|███████▊  | 133857280/170498071 [00:07<00:07, 4810410.01it/s] 79%|███████▉  | 134348800/170498071 [00:07<00:07, 4804024.80it/s] 79%|███████▉  | 134840320/170498071 [00:07<00:07, 4732245.24it/s] 79%|███████▉  | 135331840/170498071 [00:08<00:07, 4760586.54it/s] 80%|███████▉  | 135856128/170498071 [00:08<00:07, 4814958.23it/s] 80%|███████▉  | 136347648/170498071 [00:08<00:07, 4839545.50it/s] 80%|████████  | 136871936/170498071 [00:08<00:06, 4894490.29it/s] 81%|████████  | 137396224/170498071 [00:08<00:06, 4960326.60it/s] 81%|████████  | 137920512/170498071 [00:08<00:06, 4944878.23it/s] 81%|████████  | 138444800/170498071 [00:08<00:06, 5007868.65it/s] 82%|████████▏ | 139001856/170498071 [00:08<00:06, 5116291.81it/s] 82%|████████▏ | 139526144/170498071 [00:08<00:06, 5151734.08it/s] 82%|████████▏ | 140083200/170498071 [00:09<00:05, 5225931.27it/s] 82%|████████▏ | 140607488/170498071 [00:09<00:05, 5225692.22it/s] 83%|████████▎ | 141164544/170498071 [00:09<00:05, 5267618.05it/s] 83%|████████▎ | 141721600/170498071 [00:09<00:05, 5296687.20it/s] 83%|████████▎ | 142278656/170498071 [00:09<00:05, 5296327.76it/s] 84%|████████▍ | 142868480/170498071 [00:09<00:05, 5380974.81it/s] 84%|████████▍ | 143425536/170498071 [00:09<00:04, 5431493.24it/s] 84%|████████▍ | 144015360/170498071 [00:09<00:04, 5539844.52it/s] 85%|████████▍ | 144572416/170498071 [00:09<00:04, 5548457.43it/s] 85%|████████▌ | 145162240/170498071 [00:09<00:04, 5596005.49it/s] 85%|████████▌ | 145752064/170498071 [00:10<00:04, 5634689.63it/s] 86%|████████▌ | 146341888/170498071 [00:10<00:04, 5708991.14it/s] 86%|████████▌ | 146964480/170498071 [00:10<00:04, 5809587.21it/s] 87%|████████▋ | 147587072/170498071 [00:10<00:03, 5851889.34it/s] 87%|████████▋ | 148209664/170498071 [00:10<00:03, 5949142.17it/s] 87%|████████▋ | 148865024/170498071 [00:10<00:03, 6031680.57it/s] 88%|████████▊ | 149520384/170498071 [00:10<00:03, 6112121.39it/s] 88%|████████▊ | 150175744/170498071 [00:10<00:03, 6238525.10it/s] 88%|████████▊ | 150831104/170498071 [00:10<00:03, 6258902.46it/s] 89%|████████▉ | 151486464/170498071 [00:10<00:03, 6291895.15it/s] 89%|████████▉ | 152141824/170498071 [00:11<00:02, 6302615.61it/s] 90%|████████▉ | 152797184/170498071 [00:11<00:02, 6355807.74it/s] 90%|█████████ | 153485312/170498071 [00:11<00:02, 6457044.92it/s] 90%|█████████ | 154173440/170498071 [00:11<00:02, 6521107.74it/s] 91%|█████████ | 154861568/170498071 [00:11<00:02, 6601712.38it/s] 91%|█████████ | 155549696/170498071 [00:11<00:02, 6611005.35it/s] 92%|█████████▏| 156237824/170498071 [00:11<00:02, 6654180.09it/s] 92%|█████████▏| 156958720/170498071 [00:11<00:01, 6781624.32it/s] 92%|█████████▏| 157646848/170498071 [00:11<00:01, 6808919.45it/s] 93%|█████████▎| 158367744/170498071 [00:11<00:01, 6891718.56it/s] 93%|█████████▎| 159121408/170498071 [00:12<00:01, 7017464.68it/s] 94%|█████████▍| 159875072/170498071 [00:12<00:01, 7105681.85it/s] 94%|█████████▍| 160595968/170498071 [00:12<00:01, 7132591.19it/s] 95%|█████████▍| 161316864/170498071 [00:12<00:01, 7140006.07it/s] 95%|█████████▌| 162037760/170498071 [00:12<00:01, 7148607.65it/s] 95%|█████████▌| 162791424/170498071 [00:12<00:01, 7197761.65it/s] 96%|█████████▌| 163545088/170498071 [00:12<00:00, 7225954.47it/s] 96%|█████████▋| 164331520/170498071 [00:12<00:00, 7367961.65it/s] 97%|█████████▋| 165150720/170498071 [00:12<00:00, 7531316.48it/s] 97%|█████████▋| 165937152/170498071 [00:13<00:00, 7589677.57it/s] 98%|█████████▊| 166756352/170498071 [00:13<00:00, 7629096.38it/s] 98%|█████████▊| 167575552/170498071 [00:13<00:00, 7707431.06it/s] 99%|█████████▉| 168394752/170498071 [00:13<00:00, 7835016.95it/s] 99%|█████████▉| 169181184/170498071 [00:13<00:00, 7617790.86it/s]100%|█████████▉| 169967616/170498071 [00:13<00:00, 6931409.86it/s]100%|██████████| 170498071/170498071 [00:13<00:00, 12482149.89it/s]
seed: 0,   max_iteration: 3418,   1 epoch ~= 49 iterations
seed: 0,   max_iteration: 3418,   1 epoch ~= 49 iterations
/local/scratch/manwa22/idp-sgd/experiments/../opacus/opacus/privacy_engine.py:126: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/local/scratch/manwa22/idp-sgd/experiments/../opacus/opacus/accountants/analysis/rdp.py:74: RuntimeWarning: invalid value encountered in scalar subtract
  return math.log1p(math.exp(a - b)) + b  # log1p(x) = log(x + 1)
Initializing privacy parameters:   max_grad_norm=1.1,   sample_rate=0.02040816326530612,   noise_multiplier 3.1876130859375,   individual max_grad_norms=[0.7124062261904763, 1.3009157173913044, 1.802021269761606]
/local/scratch/manwa22/miniconda3/envs/idp/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
iteration: 0,   train accuracy: 10.22,   test accuracy: 10.35,   loss: 2.30016,   DP costs: [0.03, 0.12, 0.254],   average batch-size: 942,   best alpha: [187.04, 56.35, 29.67],   time: 1
iteration: 100,   train accuracy: 29.4,   test accuracy: 30.0,   loss: 2.00576,   DP costs: [0.154, 0.315, 0.487],   average batch-size: 1030,   best alpha: [83.75, 43.86, 27.73],   time: 35
iteration: 200,   train accuracy: 35.97,   test accuracy: 37.28,   loss: 1.84067,   DP costs: [0.22, 0.446, 0.678],   average batch-size: 1022,   best alpha: [62.24, 33.24, 22.66],   time: 68
iteration: 300,   train accuracy: 41.72,   test accuracy: 42.42,   loss: 1.85475,   DP costs: [0.272, 0.549, 0.83],   average batch-size: 1014,   best alpha: [52.07, 28.11, 19.44],   time: 100
iteration: 400,   train accuracy: 43.57,   test accuracy: 44.56,   loss: 1.84039,   DP costs: [0.317, 0.637, 0.96],   average batch-size: 1023,   best alpha: [45.95, 24.96, 17.36],   time: 131
iteration: 500,   train accuracy: 45.87,   test accuracy: 45.99,   loss: 1.65372,   DP costs: [0.357, 0.716, 1.077],   average batch-size: 1018,   best alpha: [41.71, 22.7, 15.94],   time: 164
iteration: 600,   train accuracy: 46.75,   test accuracy: 46.91,   loss: 1.5207,   DP costs: [0.393, 0.787, 1.183],   average batch-size: 1026,   best alpha: [38.33, 21.07, 14.82],   time: 196
iteration: 700,   train accuracy: 48.23,   test accuracy: 48.09,   loss: 1.31094,   DP costs: [0.427, 0.854, 1.282],   average batch-size: 1010,   best alpha: [35.81, 19.71, 13.93],   time: 228
iteration: 800,   train accuracy: 49.06,   test accuracy: 49.02,   loss: 1.43513,   DP costs: [0.458, 0.916, 1.374],   average batch-size: 1016,   best alpha: [33.85, 18.66, 13.21],   time: 260
iteration: 900,   train accuracy: 49.01,   test accuracy: 48.48,   loss: 1.53232,   DP costs: [0.488, 0.975, 1.462],   average batch-size: 1025,   best alpha: [32.14, 17.74, 12.57],   time: 292
iteration: 1000,   train accuracy: 50.74,   test accuracy: 50.72,   loss: 1.49149,   DP costs: [0.516, 1.031, 1.545],   average batch-size: 1021,   best alpha: [30.63, 16.99, 12.06],   time: 324
iteration: 1100,   train accuracy: 51.86,   test accuracy: 51.64,   loss: 1.70012,   DP costs: [0.543, 1.084, 1.625],   average batch-size: 1010,   best alpha: [29.32, 16.28, 11.61],   time: 355
iteration: 1200,   train accuracy: 53.22,   test accuracy: 53.3,   loss: 1.47563,   DP costs: [0.568, 1.136, 1.701],   average batch-size: 1029,   best alpha: [28.18, 15.73, 11.23],   time: 388
iteration: 1300,   train accuracy: 54.19,   test accuracy: 53.75,   loss: 1.24457,   DP costs: [0.593, 1.185, 1.775],   average batch-size: 1014,   best alpha: [27.19, 15.19, 10.85],   time: 419
iteration: 1400,   train accuracy: 54.8,   test accuracy: 54.78,   loss: 1.23472,   DP costs: [0.617, 1.233, 1.847],   average batch-size: 1012,   best alpha: [26.34, 14.73, 10.53],   time: 451
iteration: 1500,   train accuracy: 54.96,   test accuracy: 54.8,   loss: 1.28949,   DP costs: [0.641, 1.279, 1.916],   average batch-size: 1025,   best alpha: [25.51, 14.29, 10.26],   time: 482
iteration: 1600,   train accuracy: 56.42,   test accuracy: 55.99,   loss: 1.44637,   DP costs: [0.663, 1.324, 1.983],   average batch-size: 1004,   best alpha: [24.82, 13.91, 10.0],   time: 513
iteration: 1700,   train accuracy: 57.02,   test accuracy: 56.23,   loss: 1.29762,   DP costs: [0.685, 1.368, 2.049],   average batch-size: 1024,   best alpha: [24.14, 13.54, 9.74],   time: 546
iteration: 1800,   train accuracy: 56.35,   test accuracy: 56.41,   loss: 1.22861,   DP costs: [0.706, 1.411, 2.113],   average batch-size: 1043,   best alpha: [23.57, 13.23, 9.53],   time: 577
iteration: 1900,   train accuracy: 56.55,   test accuracy: 55.76,   loss: 1.5897,   DP costs: [0.727, 1.452, 2.175],   average batch-size: 1028,   best alpha: [23.02, 12.93, 9.32],   time: 609
iteration: 2000,   train accuracy: 56.71,   test accuracy: 56.72,   loss: 1.36222,   DP costs: [0.747, 1.493, 2.236],   average batch-size: 1023,   best alpha: [22.48, 12.64, 9.12],   time: 640
iteration: 2100,   train accuracy: 56.75,   test accuracy: 57.23,   loss: 1.71232,   DP costs: [0.767, 1.532, 2.296],   average batch-size: 1030,   best alpha: [22.05, 12.41, 8.95],   time: 672
iteration: 2200,   train accuracy: 57.59,   test accuracy: 56.84,   loss: 1.36896,   DP costs: [0.787, 1.571, 2.354],   average batch-size: 994,   best alpha: [21.53, 12.17, 8.79],   time: 703
iteration: 2300,   train accuracy: 57.41,   test accuracy: 56.47,   loss: 1.28684,   DP costs: [0.806, 1.61, 2.412],   average batch-size: 1025,   best alpha: [21.11, 11.94, 8.63],   time: 735
iteration: 2400,   train accuracy: 57.67,   test accuracy: 57.39,   loss: 1.35219,   DP costs: [0.824, 1.647, 2.468],   average batch-size: 1025,   best alpha: [20.78, 11.72, 8.47],   time: 767
iteration: 2500,   train accuracy: 57.69,   test accuracy: 57.27,   loss: 1.25873,   DP costs: [0.843, 1.684, 2.523],   average batch-size: 1009,   best alpha: [20.38, 11.5, 8.35],   time: 798
iteration: 2600,   train accuracy: 58.21,   test accuracy: 57.0,   loss: 1.34514,   DP costs: [0.861, 1.72, 2.578],   average batch-size: 1018,   best alpha: [19.98, 11.33, 8.23],   time: 830
iteration: 2700,   train accuracy: 58.91,   test accuracy: 57.25,   loss: 1.2306,   DP costs: [0.878, 1.755, 2.631],   average batch-size: 1015,   best alpha: [19.67, 11.16, 8.08],   time: 862
iteration: 2800,   train accuracy: 58.07,   test accuracy: 57.19,   loss: 1.46983,   DP costs: [0.896, 1.79, 2.684],   average batch-size: 1008,   best alpha: [19.37, 11.0, 7.97],   time: 894
iteration: 2900,   train accuracy: 58.54,   test accuracy: 57.1,   loss: 1.32962,   DP costs: [0.913, 1.825, 2.736],   average batch-size: 1006,   best alpha: [19.07, 10.83, 7.88],   time: 925
iteration: 3000,   train accuracy: 59.21,   test accuracy: 57.42,   loss: 1.43185,   DP costs: [0.93, 1.859, 2.787],   average batch-size: 1034,   best alpha: [18.77, 10.67, 7.77],   time: 957
iteration: 3100,   train accuracy: 59.55,   test accuracy: 58.1,   loss: 1.29299,   DP costs: [0.946, 1.892, 2.837],   average batch-size: 1009,   best alpha: [18.55, 10.51, 7.66],   time: 989
iteration: 3200,   train accuracy: 59.23,   test accuracy: 57.97,   loss: 1.13411,   DP costs: [0.963, 1.925, 2.887],   average batch-size: 1024,   best alpha: [18.27, 10.4, 7.58],   time: 1021
iteration: 3300,   train accuracy: 59.28,   test accuracy: 57.97,   loss: 1.50523,   DP costs: [0.979, 1.958, 2.936],   average batch-size: 1037,   best alpha: [17.98, 10.24, 7.47],   time: 1053
iteration: 3400,   train accuracy: 58.26,   test accuracy: 57.19,   loss: 1.70573,   DP costs: [0.995, 1.99, 2.985],   average batch-size: 1015,   best alpha: [17.77, 10.13, 7.39],   time: 1085
iteration: 3417,   train accuracy: 58.11,   test accuracy: 57.52,   loss: 0.92066,   DP costs: [0.997, 1.995, 2.993],   average batch-size: 997,   best alpha: [17.77, 10.09, 7.36],   time: 1093
Terminate: The maximum of iterations is reached!
