Input args: Namespace(dname='CIFAR10', seeds=[0], architecture='CIFAR10_CNN', accountant='rdp', individualize='sampling', log_iteration=100, lr=0.2, momentum=0.5, epochs=60, n_workers=6, batch_size=1024, max_physical_batch_size=128, delta=1e-05, budgets=[1.0, 2.0, 3.0], ratios=[0.34, 0.43, 0.23], max_grad_norm=1.0, noise_multiplier=2.65712, weights=None, adapt_weights_to_budgets=True, use_cuda='True', save_path='../cifar_results/sampling/CIFAR10/epochs_60_batch_1024_lr_0.2_max_grad_norm_1.0_budgets_1.0_2.0_3.0_ratios_0.34_0.43_0.23_seeds_0', mode='mia', accuracy_log='accuracy.log', assign_budget='even', class_budgets=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], mia_ndata=50000, mia_count=0, allow_excess=False, save_model='True')
seed: 0,   max_iteration: 2930,   1 epoch ~= 49 iterations
seed: 0,   max_iteration: 2930,   1 epoch ~= 49 iterations
/local/scratch/manwa22/idp-sgd/experiments/../opacus/opacus/privacy_engine.py:126: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/local/scratch/manwa22/idp-sgd/experiments/../opacus/opacus/accountants/analysis/rdp.py:74: RuntimeWarning: invalid value encountered in scalar subtract
  return math.log1p(math.exp(a - b)) + b  # log1p(x) = log(x + 1)
Initializing privacy parameters:   max_grad_norm=1.0,   sample_rate=0.02040816326530612,   noise_multiplier 2.5374942626953128,   individual sample_rates=[0.01094970792175293, 0.020617676575073244, 0.029687500703125]
/local/scratch/manwa22/miniconda3/envs/idp/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
iteration: 0,   train accuracy: 10.47,   test accuracy: 10.4,   loss: 2.30267,   DP costs: [0.114, 0.138, 0.158],   average batch-size: 972,   best alpha: [58.13, 49.92, 45.04],   time: 1
iteration: 100,   train accuracy: 28.27,   test accuracy: 28.92,   loss: 2.03825,   DP costs: [0.181, 0.344, 0.509],   average batch-size: 1026,   best alpha: [56.97, 40.3, 28.98],   time: 33
iteration: 200,   train accuracy: 33.29,   test accuracy: 34.3,   loss: 1.98111,   DP costs: [0.245, 0.486, 0.72],   average batch-size: 1034,   best alpha: [55.15, 30.81, 22.14],   time: 65
iteration: 300,   train accuracy: 38.19,   test accuracy: 38.65,   loss: 1.80526,   DP costs: [0.301, 0.597, 0.886],   average batch-size: 1007,   best alpha: [46.73, 26.18, 18.78],   time: 97
iteration: 400,   train accuracy: 41.36,   test accuracy: 42.44,   loss: 1.59629,   DP costs: [0.35, 0.693, 1.029],   average batch-size: 1016,   best alpha: [41.4, 23.25, 16.71],   time: 129
iteration: 500,   train accuracy: 43.56,   test accuracy: 43.86,   loss: 1.59478,   DP costs: [0.392, 0.778, 1.156],   average batch-size: 1023,   best alpha: [37.75, 21.15, 15.29],   time: 160
iteration: 600,   train accuracy: 44.81,   test accuracy: 44.91,   loss: 1.56719,   DP costs: [0.431, 0.856, 1.272],   average batch-size: 1031,   best alpha: [34.84, 19.63, 14.15],   time: 192
iteration: 700,   train accuracy: 46.14,   test accuracy: 46.08,   loss: 1.76458,   DP costs: [0.468, 0.928, 1.38],   average batch-size: 1018,   best alpha: [32.68, 18.37, 13.31],   time: 223
iteration: 800,   train accuracy: 46.96,   test accuracy: 46.98,   loss: 1.4464,   DP costs: [0.501, 0.995, 1.481],   average batch-size: 1010,   best alpha: [30.78, 17.4, 12.58],   time: 253
iteration: 900,   train accuracy: 47.56,   test accuracy: 47.05,   loss: 1.57207,   DP costs: [0.533, 1.059, 1.577],   average batch-size: 1034,   best alpha: [29.34, 16.54, 11.97],   time: 284
iteration: 1000,   train accuracy: 48.14,   test accuracy: 48.02,   loss: 1.62267,   DP costs: [0.564, 1.12, 1.668],   average batch-size: 1010,   best alpha: [27.97, 15.85, 11.49],   time: 315
iteration: 1100,   train accuracy: 49.45,   test accuracy: 49.11,   loss: 1.7417,   DP costs: [0.593, 1.178, 1.756],   average batch-size: 1021,   best alpha: [26.88, 15.19, 11.02],   time: 346
iteration: 1200,   train accuracy: 50.08,   test accuracy: 50.05,   loss: 1.59197,   DP costs: [0.621, 1.234, 1.84],   average batch-size: 1028,   best alpha: [25.94, 14.68, 10.66],   time: 377
