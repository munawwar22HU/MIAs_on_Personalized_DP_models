Input args: Namespace(dname='CIFAR10', seeds=[0], architecture='CIFAR10_CNN', accountant='rdp', individualize='sampling', log_iteration=100, lr=0.1, momentum=0.5, epochs=60, n_workers=6, batch_size=1024, max_physical_batch_size=128, delta=1e-05, budgets=[1.0, 2.0, 3.0], ratios=[0.54, 0.37, 0.09], max_grad_norm=1.8, noise_multiplier=3.14049, weights=None, adapt_weights_to_budgets=True, use_cuda='True', save_path='../cifar_results/sampling/CIFAR10/epochs_60_batch_1024_lr_0.1_max_grad_norm_1.8_budgets_1.0_2.0_3.0_ratios_0.54_0.37_0.09_seeds_0', mode='mia', accuracy_log='accuracy.log', assign_budget='even', class_budgets=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], mia_ndata=50000, mia_count=0, allow_excess=False, save_model='True')
seed: 0,   max_iteration: 2930,   1 epoch ~= 49 iterations
seed: 0,   max_iteration: 2930,   1 epoch ~= 49 iterations
/local/scratch/manwa22/idp-sgd/experiments/../opacus/opacus/privacy_engine.py:126: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/local/scratch/manwa22/idp-sgd/experiments/../opacus/opacus/accountants/analysis/rdp.py:74: RuntimeWarning: invalid value encountered in scalar subtract
  return math.log1p(math.exp(a - b)) + b  # log1p(x) = log(x + 1)
Initializing privacy parameters:   max_grad_norm=1.8,   sample_rate=0.02040816326530612,   noise_multiplier 2.5374942626953128,   individual sample_rates=[0.01094970792175293, 0.020617676575073244, 0.029687500703125]
/local/scratch/manwa22/miniconda3/envs/idp/lib/python3.9/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
iteration: 0,   train accuracy: 10.42,   test accuracy: 10.45,   loss: 2.30267,   DP costs: [0.114, 0.138, 0.158],   average batch-size: 972,   best alpha: [58.13, 49.92, 45.04],   time: 1
iteration: 100,   train accuracy: 27.5,   test accuracy: 28.29,   loss: 2.04913,   DP costs: [0.181, 0.344, 0.509],   average batch-size: 1026,   best alpha: [56.97, 40.3, 28.98],   time: 30
iteration: 200,   train accuracy: 31.65,   test accuracy: 32.63,   loss: 2.00478,   DP costs: [0.245, 0.486, 0.72],   average batch-size: 1034,   best alpha: [55.15, 30.81, 22.14],   time: 58
iteration: 300,   train accuracy: 36.5,   test accuracy: 36.85,   loss: 1.85982,   DP costs: [0.301, 0.597, 0.886],   average batch-size: 1007,   best alpha: [46.73, 26.18, 18.78],   time: 85
iteration: 400,   train accuracy: 39.85,   test accuracy: 40.87,   loss: 1.65682,   DP costs: [0.35, 0.693, 1.029],   average batch-size: 1016,   best alpha: [41.4, 23.25, 16.71],   time: 111
iteration: 500,   train accuracy: 42.53,   test accuracy: 43.11,   loss: 1.61767,   DP costs: [0.392, 0.778, 1.156],   average batch-size: 1023,   best alpha: [37.75, 21.15, 15.29],   time: 138
iteration: 600,   train accuracy: 43.69,   test accuracy: 44.06,   loss: 1.55218,   DP costs: [0.431, 0.856, 1.272],   average batch-size: 1031,   best alpha: [34.84, 19.63, 14.15],   time: 165
iteration: 700,   train accuracy: 45.21,   test accuracy: 45.04,   loss: 1.73298,   DP costs: [0.468, 0.928, 1.38],   average batch-size: 1018,   best alpha: [32.68, 18.37, 13.31],   time: 192
iteration: 800,   train accuracy: 45.91,   test accuracy: 45.92,   loss: 1.44854,   DP costs: [0.501, 0.995, 1.481],   average batch-size: 1010,   best alpha: [30.78, 17.4, 12.58],   time: 218
iteration: 900,   train accuracy: 46.72,   test accuracy: 46.42,   loss: 1.52576,   DP costs: [0.533, 1.059, 1.577],   average batch-size: 1034,   best alpha: [29.34, 16.54, 11.97],   time: 245
iteration: 1000,   train accuracy: 47.45,   test accuracy: 47.22,   loss: 1.61731,   DP costs: [0.564, 1.12, 1.668],   average batch-size: 1010,   best alpha: [27.97, 15.85, 11.49],   time: 272
iteration: 1100,   train accuracy: 48.66,   test accuracy: 47.89,   loss: 1.68299,   DP costs: [0.593, 1.178, 1.756],   average batch-size: 1021,   best alpha: [26.88, 15.19, 11.02],   time: 298
iteration: 1200,   train accuracy: 49.16,   test accuracy: 49.0,   loss: 1.57506,   DP costs: [0.621, 1.234, 1.84],   average batch-size: 1028,   best alpha: [25.94, 14.68, 10.66],   time: 325
iteration: 1300,   train accuracy: 50.28,   test accuracy: 49.78,   loss: 1.27197,   DP costs: [0.647, 1.288, 1.921],   average batch-size: 1012,   best alpha: [25.03, 14.18, 10.3],   time: 351
iteration: 1400,   train accuracy: 50.08,   test accuracy: 50.02,   loss: 1.44651,   DP costs: [0.673, 1.34, 1.999],   average batch-size: 1017,   best alpha: [24.25, 13.75, 10.0],   time: 378
iteration: 1500,   train accuracy: 51.33,   test accuracy: 50.91,   loss: 1.31692,   DP costs: [0.698, 1.39, 2.075],   average batch-size: 1035,   best alpha: [23.5, 13.34, 9.71],   time: 405
iteration: 1600,   train accuracy: 51.89,   test accuracy: 51.97,   loss: 1.30384,   DP costs: [0.723, 1.439, 2.148],   average batch-size: 1005,   best alpha: [22.86, 12.99, 9.46],   time: 432
iteration: 1700,   train accuracy: 53.07,   test accuracy: 52.32,   loss: 1.428,   DP costs: [0.746, 1.486, 2.22],   average batch-size: 1025,   best alpha: [22.24, 12.64, 9.22],   time: 459
iteration: 1800,   train accuracy: 52.85,   test accuracy: 53.31,   loss: 1.35594,   DP costs: [0.769, 1.533, 2.29],   average batch-size: 1032,   best alpha: [21.72, 12.36, 9.02],   time: 486
iteration: 1900,   train accuracy: 54.13,   test accuracy: 53.76,   loss: 1.41443,   DP costs: [0.792, 1.578, 2.359],   average batch-size: 1024,   best alpha: [21.21, 12.08, 8.82],   time: 515
iteration: 2000,   train accuracy: 54.35,   test accuracy: 54.31,   loss: 1.40648,   DP costs: [0.814, 1.622, 2.426],   average batch-size: 1033,   best alpha: [20.72, 11.81, 8.63],   time: 546
iteration: 2100,   train accuracy: 54.89,   test accuracy: 55.1,   loss: 1.31778,   DP costs: [0.835, 1.665, 2.491],   average batch-size: 1027,   best alpha: [20.31, 11.59, 8.48],   time: 577
iteration: 2200,   train accuracy: 56.13,   test accuracy: 55.7,   loss: 1.4714,   DP costs: [0.856, 1.708, 2.555],   average batch-size: 1000,   best alpha: [19.92, 11.37, 8.32],   time: 607
iteration: 2300,   train accuracy: 55.97,   test accuracy: 55.26,   loss: 1.44739,   DP costs: [0.876, 1.749, 2.618],   average batch-size: 1017,   best alpha: [19.53, 11.16, 8.18],   time: 638
iteration: 2400,   train accuracy: 56.58,   test accuracy: 55.76,   loss: 1.30018,   DP costs: [0.897, 1.79, 2.68],   average batch-size: 1028,   best alpha: [19.16, 10.96, 8.03],   time: 669
iteration: 2500,   train accuracy: 56.52,   test accuracy: 56.38,   loss: 1.24938,   DP costs: [0.916, 1.83, 2.741],   average batch-size: 1013,   best alpha: [18.86, 10.75, 7.91],   time: 700
iteration: 2600,   train accuracy: 57.3,   test accuracy: 56.58,   loss: 1.29937,   DP costs: [0.936, 1.869, 2.8],   average batch-size: 1018,   best alpha: [18.49, 10.59, 7.77],   time: 730
iteration: 2700,   train accuracy: 58.08,   test accuracy: 57.28,   loss: 1.37446,   DP costs: [0.955, 1.908, 2.859],   average batch-size: 1016,   best alpha: [18.21, 10.44, 7.66],   time: 761
iteration: 2800,   train accuracy: 57.76,   test accuracy: 57.22,   loss: 1.38876,   DP costs: [0.974, 1.946, 2.917],   average batch-size: 1017,   best alpha: [17.93, 10.28, 7.55],   time: 792
iteration: 2900,   train accuracy: 58.41,   test accuracy: 57.48,   loss: 1.3874,   DP costs: [0.992, 1.983, 2.974],   average batch-size: 1006,   best alpha: [17.65, 10.13, 7.45],   time: 823
iteration: 2929,   train accuracy: 58.77,   test accuracy: 57.63,   loss: 1.38992,   DP costs: [0.997, 1.994, 2.99],   average batch-size: 1010,   best alpha: [17.58, 10.09, 7.42],   time: 834
Terminate: The maximum of iterations is reached!
